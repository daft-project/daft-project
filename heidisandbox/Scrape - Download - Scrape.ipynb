{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daft Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get individual rental ad  URLs\n",
    "Pagination is done using the 'offset' property in the search results URL, so we can use that browse through the results pages.  \n",
    "Daft displays 20 results per page, hence the value of '20' in this line:\n",
    "       `for offset in range(0, number_of_adds, 20):`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of Dublin rental ads: 634\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "daftresults_urlroot = 'http://www.daft.ie/dublin/apartments-for-rent/?s%5Bignored_agents%5D%5B0%5D=5732&s%5Bignored_agents%5D%5B1%5D=428&s%5Bignored_agents%5D%5B2%5D=1551&offset='\n",
    "allAdUrls = []\n",
    "number_of_adds = 700\n",
    "\n",
    "def getAdUrls(pageresults):\n",
    "    \n",
    "    adURLs = []\n",
    "    for result in pageresults:\n",
    "        adURLs.append(\"http://www.daft.ie\" + result.a[\"href\"])\n",
    "    return adURLs\n",
    "\n",
    "for offset in range(0, number_of_adds, 20):\n",
    "    results_html = urllib.request.urlopen(daftresults_urlroot + str(offset)).read()\n",
    "    soup = BeautifulSoup(results_html, \"html5lib\")\n",
    "    results = soup.find_all(\"div\", class_=\"search_result_title_box\")\n",
    "    allAdUrls = allAdUrls + getAdUrls(results)\n",
    "\n",
    "#print(allAdUrls)\n",
    "print('Current number of Dublin rental ads: ' + str(len(allAdUrls)))\n",
    "number_of_adds = len(allAdUrls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download individual Ad pages\n",
    "\n",
    "This section loops through the list of individual rental ad URLs, and downloads them into a 'daftpages' directory. \n",
    "\n",
    "#### Skip if you download and extract the zipped/tared archive instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rental Ad URLs are now in this array: allAdUrls\n",
    "# Loop through and download\n",
    "\n",
    "for idx,adUrl in enumerate(allAdUrls):\n",
    "    urllib.request.urlretrieve(adUrl, 'daftpages/daft_ad_' + str(idx) + '.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip up pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "current_date = datetime.datetime.now().isoformat()\n",
    "tar_file_name = 'daftpages_' + current_date + '.tar'\n",
    "source_dir = 'daftpages/'\n",
    "with tarfile.open(tar_file_name, \"w:gz\") as tar:\n",
    "    tar.add(source_dir, arcname=os.path.basename(source_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "num_of_rows = number_of_adds\n",
    "data_csv = 'data/scraped_data.csv'\n",
    "all_orig_field_names = [\n",
    "    'property_id',\n",
    "    'property_category',\n",
    "    'property_title',\n",
    "    'property_type',\n",
    "    'seller_name',\n",
    "    'seller_id',\n",
    "    'seller_type',\n",
    "    'open_viewing',\n",
    "    'no_of_photos',\n",
    "    'available_from',\n",
    "    'lease_units',\n",
    "    'available_for',    \n",
    "    'area',\n",
    "    'county',\n",
    "    'latitude',\n",
    "    'longitude',    \n",
    "    'furnished',\n",
    "    'bathrooms',   \n",
    "    'beds',   \n",
    "    'facility',    \n",
    "    'environment',\n",
    "    'published_date',\n",
    "    'page_name',\n",
    "    'platform',\n",
    "    'currency',\n",
    "    'price_frequency',\n",
    "    'price'\n",
    "]\n",
    "all_facilities=[\n",
    "    'Parking', \n",
    "    'Cable Television', \n",
    "    'Dryer', \n",
    "    'Garden / Patio / Balcony', \n",
    "    'Washing Machine', \n",
    "    'Serviced Property', \n",
    "    'Pets Allowed', \n",
    "    'Wheelchair Access', \n",
    "    'Central Heating', \n",
    "    'Microwave', \n",
    "    'Smoking', \n",
    "    'Dishwasher', \n",
    "    'House Alarm', \n",
    "    'Internet'\n",
    "]\n",
    "\n",
    "with open(data_csv, 'w') as csvfile:\n",
    "    for idx in range(num_of_rows):\n",
    "        try:\n",
    "            adpage_html = open('daftpages/daft_ad_' + str(idx) + '.html').read()\n",
    "            soup = BeautifulSoup(adpage_html, \"html5lib\")\n",
    "        except:\n",
    "            # seems like some pages have encoding issues?\n",
    "            print('issue reading in page daftpages/daft_ad_' + str(idx) + '.html. Skipping this Ad.')\n",
    "            continue\n",
    "\n",
    "        #print(soup)\n",
    "        # There is a handy javascrupt json dictionary on those daft pages, listing key features of the add\n",
    "        # To get this data, find all script tags, then get the contents of the 10ths tag found (seems to be the 10th.\n",
    "        # Now, this seems to be a bit brittle, need to find a way to target this better than just hope it'll always be \n",
    "        # the 10th script tag on the page; But maybe for now it's enough)\n",
    "        scriptdata = soup.find_all('script', type='text/javascript')    \n",
    "        trackingparams = scriptdata[10].get_text()\n",
    "        trackingparams = trackingparams.replace('\\u20ac','')\n",
    "\n",
    "        try:\n",
    "            feature_str = \"{\" + str(re.search('\\\\{(.+?)\\\\}', trackingparams).group(1)) + \"}\"\n",
    "        except AttributeError:\n",
    "            feature_str = \"{}\"\n",
    "    \n",
    "        ad_data = json.loads(feature_str)  \n",
    "        \n",
    "        field_names = ad_data.keys()\n",
    "        \n",
    "        facilities = ad_data['facility'].split(',')\n",
    "        facilties_dict = dict.fromkeys(all_facilities)\n",
    "        for facility in facilities:\n",
    "            if facility in all_facilities:\n",
    "                facilties_dict[facility] = True\n",
    " \n",
    "\n",
    "        # check for missing fields (mostly seller_id and seller_name), and add them with empty vals if required\n",
    "        missing_fiels = set(all_orig_field_names) - set(field_names)\n",
    "        for missing in missing_fiels:\n",
    "            ad_data[missing] = \"\"\n",
    "        \n",
    "        ad_data.update(facilties_dict)\n",
    "        \n",
    "        all_field_names = all_orig_field_names + all_facilities\n",
    "        \n",
    "        writer = csv.DictWriter(csvfile, fieldnames=all_field_names)\n",
    "        if idx == 0: \n",
    "            writer.writeheader()\n",
    "        writer.writerow(ad_data)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Create Pandas Dataframe from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_csv = 'data/scraped_data.csv'\n",
    "df = pd.read_csv(data_csv)\n",
    "\n",
    "#drop some not very useful columns\n",
    "df = df.drop('environment', 1)\n",
    "df = df.drop('page_name', 1)\n",
    "df = df.drop('platform', 1)\n",
    "df = df.drop('property_category', 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
